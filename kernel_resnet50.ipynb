{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import os\n\n#Imports for constructing the Neural Network\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision\n\n#Imports for handling data\nimport numpy as np\nimport pandas as pd\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\n\n#Imports for visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n\nimport cv2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input/\"))\n\n#labels for the training set images\nlabels = pd.read_csv('../input/train.csv')\n#path to training set\ntrain_dir = \"../input/train/train\"\n#path to testing set\ntest_dir = \"../input/test/test\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Device with the hardware where the model will be trained."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Copied from \"Detecting cactus with kekas\" notebook\nfig = plt.figure(figsize=(25, 8))\ntrain_imgs = os.listdir(\"../input/train/train\")\nfor idx, img in enumerate(np.random.choice(train_imgs, 20)):\n    ax = fig.add_subplot(4, 20//4, idx+1, xticks=[], yticks=[])\n    im = Image.open(\"../input/train/train/\" + img)\n    plt.imshow(im)\n    lab = labels.loc[labels['id'] == img, 'has_cactus'].values[0]\n    ax.set_title(f'Label: {lab}')\nprint(im.size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Images are 32x32\nBoth vertical and horizontal flips are acceptable, as well as rotation of the images."},{"metadata":{"trusted":true},"cell_type":"code","source":"labels.has_cactus.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only 1/4 of the data has no cactus in it, we can use weights in the loss function in order to punish more the misclassification when there is no cactus."},{"metadata":{"trusted":true},"cell_type":"code","source":"#From the \"SImple CNN on PyTorch for beginers\" notebook\nclass CactusDataset(Dataset):\n    def __init__(self, df_data, data_dir = './', transform=None):\n        super().__init__()\n        self.df = df_data.values\n        self.data_dir = data_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        img_name,label = self.df[index]\n        img_path = os.path.join(self.data_dir, img_name)\n        image = cv2.imread(img_path)\n        if self.transform is not None:\n            image = self.transform(image)\n        return image, label\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ratio of the training set that will be used for validation\nval_ratio = 0.15\nbatch_size =64\n\n# data splitting\ntrain, val = train_test_split(labels, stratify=labels.has_cactus, test_size=val_ratio)\ntrain.shape, val.shape\n\n# Image preprocessing\ntrain_transforms = transforms.Compose([transforms.ToPILImage(),\n                                  transforms.RandomHorizontalFlip(),\n                                  transforms.RandomVerticalFlip(),\n                                  transforms.RandomRotation(10),\n                                  transforms.ToTensor(),\n                                  transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])])\n\nval_transforms = transforms.Compose([transforms.ToPILImage(),\n                                  transforms.ToTensor(),\n                                  transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])])\n\n# Data generators\ntrain_dataset = CactusDataset(df_data=train, data_dir=train_dir, transform=train_transforms)\nval_dataset = CactusDataset(df_data=val, data_dir=train_dir, transform=val_transforms)\n\ntrain_loader = DataLoader(dataset = train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\nval_loader = DataLoader(dataset = val_dataset, batch_size=batch_size//2, shuffle=False, num_workers=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_model(model, epochs=0, val_loss=-1):\n  '''function to save the state of the model, the state of the optimizer, number of current epochs and validation loss'''\n  \n  checkpoint = {'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), \n              'epochs': epochs, 'val_loss': val_loss}\n  torch.save(checkpoint,'../checkpoint.pt')\n  print(\"Model Saved\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_model(checkpoint):\n    '''function to load a model and a optimizer with the respective state dicts,\n    returning them and the lowest validation loss associated with them'''\n    model = torchvision.models.resnet50(pretrained=False).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    \n    model.load_state_dict(checkpoint['model_state_dict'])\n    val_loss = checkpoint['val_loss']\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    \n    return model, optimizer, val_loss","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"def train(model, optimizer, criterion, scheduler, epochs, valid_loss_min):\n    # number of epochs to train the model\n    n_epochs = epochs\n    # number of epochs without improvement that triggers an early stop in trainning\n    early_stop = 12\n    es_counter = 0\n\n    for epoch in range(1, n_epochs+1):\n        # keep track of training and validation loss\n        train_loss = 0.0\n        valid_loss = 0.0\n        accuracy=0.0\n        ###################\n        # train the model #\n        ###################\n        model.train()\n        for batch_number, (data, target) in enumerate(train_loader):\n            data = data.to(device)\n            target = target.to(device)\n            # move tensors to GPU if CUDA is available\n            # clear the gradients of all optimized variables\n            optimizer.zero_grad()\n            # forward pass: compute predicted outputs by passing inputs to the model\n            output = model(data)\n            # calculate the batch loss\n            loss = criterion(output, target)\n            # backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n            # perform a single optimization step (parameter update)\n            optimizer.step()\n            # update training loss\n            train_loss += loss.item()*data.size(0)\n            #give feedback on current batch of the epoch\n            #if batch_number%50 == 0:\n                #print(\"batch number: {}\".format(batch_number))\n\n        ######################    \n        # validate the model #\n        ######################\n        model.eval()\n        for data, target in val_loader:\n            data = data.to(device)\n            target = target.to(device)\n            # move tensors to GPU if CUDA is available\n            # forward pass: compute predicted outputs by passing inputs to the model\n            output = model(data)\n            # calculate the batch loss\n            loss = criterion(output, target)\n            # update average validation loss \n            valid_loss += loss.item()*data.size(0)\n            _,pred=torch.max(output,1)\n            accuracy += torch.sum(pred==target.data)\n\n        # calculate average losses\n        train_loss = train_loss/len(train_loader.dataset)\n        valid_loss = valid_loss/len(val_loader.dataset)\n        accuracy = accuracy.double()/len(val_loader.dataset)\n\n        # confirm if the validation loss is decreasing in order to reduce lr\n        scheduler.step(valid_loss)\n\n        # print training/validation statistics \n        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}\\tAccuracy: {:.6f}'.format(\n            epoch, train_loss, valid_loss,accuracy))\n\n        # save model if validation loss has decreased, if not increase early stop counter \n        if valid_loss < valid_loss_min:\n            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n                valid_loss_min, valid_loss))\n            save_model(model, epoch, valid_loss)\n            valid_loss_min = valid_loss\n            es_counter = 0\n        else:\n            es_counter+=1\n\n        #if certain number of epochs have passed without improvement stop the trainning\n        if es_counter >= early_stop:\n            print(\"\\n\\nEarly stop, no improvements in {} epochs\".format(early_stop))\n            break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#instatiate the models\nresnet50 = torchvision.models.resnet50(pretrained=False).to(device)\n\n#define the optimizer as Adam, give him the model's parameters and define the learning rate as 0.001\noptimizer = optim.Adam(resnet50.parameters(), lr=0.001)\n\n# track change in validation loss\nvalid_loss_min = np.Inf\n    \n#define the loss as Cross Entropy Loss\ncriterion = nn.CrossEntropyLoss()\n\n#schedule the lr to decrease by half after every 3 epochs without improvement\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=3, verbose=True, min_lr=0.00001)\n\ntrain(resnet50, optimizer, criterion, scheduler, 30, valid_loss_min)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/sample_submission.csv')\n\ndataset_test = CactusDataset(df_data=sub, data_dir=test_dir, transform=val_transforms)\ntest_loader = DataLoader(dataset = dataset_test, batch_size=32, shuffle=False, num_workers=0)\n\nbest_checkpoint = torch.load('../checkpoint.pt')\nmodel, _, _ = load_model(best_checkpoint)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()\n\npreds = []\nfor batch_number, (data, target) in enumerate(test_loader):\n    data, target = data.cuda(), target.cuda()\n    output = model(data)\n\n    pr = output[:,1].detach().cpu().numpy()\n    for i in pr:\n        preds.append(i)\n\nsub['has_cactus'] = preds\nsub.to_csv('sub.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}