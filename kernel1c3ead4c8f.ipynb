{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import os\n\n#Imports for constructing the Neural Network\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision\n\n#Imports for handling data\nimport numpy as np\nimport pandas as pd\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\n\n#Imports for visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n\nimport cv2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input/\"))\n\n#labels for the training set images\nlabels = pd.read_csv('../input/train.csv')\n#path to training set\ntrain_dir = \"../input/train/train\"\n#path to testing set\ntest_dir = \"../input/test/test\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Device with the hardware where the model will be trained."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Copied from \"Detecting cactus with kekas\" notebook\nfig = plt.figure(figsize=(25, 8))\ntrain_imgs = os.listdir(\"../input/train/train\")\nfor idx, img in enumerate(np.random.choice(train_imgs, 20)):\n    ax = fig.add_subplot(4, 20//4, idx+1, xticks=[], yticks=[])\n    im = Image.open(\"../input/train/train/\" + img)\n    plt.imshow(im)\n    lab = labels.loc[labels['id'] == img, 'has_cactus'].values[0]\n    ax.set_title(f'Label: {lab}')\nprint(im.size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Images are 32x32\nBoth vertical and horizontal flips are acceptable, as well as rotation of the images."},{"metadata":{"trusted":true},"cell_type":"code","source":"labels.has_cactus.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only 1/4 of the data has no cactus in it, we can use weights in the loss function in order to punish more the misclassification when there is no cactus."},{"metadata":{"trusted":true},"cell_type":"code","source":"#From the \"SImple CNN on PyTorch for beginers\" notebook\nclass CactusDataset(Dataset):\n    def __init__(self, df_data, data_dir = './', transform=None):\n        super().__init__()\n        self.df = df_data.values\n        self.data_dir = data_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        img_name,label = self.df[index]\n        img_path = os.path.join(self.data_dir, img_name)\n        image = cv2.imread(img_path)\n        if self.transform is not None:\n            image = self.transform(image)\n        return image, label\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ratio of the training set that will be used for validation\nval_ratio = 0.15\nbatch_size =64\n\n# data splitting\ntrain, val = train_test_split(labels, stratify=labels.has_cactus, test_size=val_ratio)\ntrain.shape, val.shape\n\n# Image preprocessing\ntrain_transforms = transforms.Compose([transforms.ToPILImage(),\n                                  transforms.RandomHorizontalFlip(),\n                                  transforms.RandomVerticalFlip(),\n                                  transforms.RandomRotation(10),\n                                  transforms.ToTensor(),\n                                  transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])\n\nval_transforms = transforms.Compose([transforms.ToPILImage(),\n                                  transforms.ToTensor(),\n                                  transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])\n\n# Data generators\ntrain_dataset = CactusDataset(df_data=train, data_dir=train_dir, transform=train_transforms)\nval_dataset = CactusDataset(df_data=val, data_dir=train_dir, transform=val_transforms)\n\ntrain_loader = DataLoader(dataset = train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\nval_loader = DataLoader(dataset = val_dataset, batch_size=batch_size//2, shuffle=False, num_workers=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CactusNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = nn.Sequential(\n          # 3-> number of channels in the input \n          # 32 -> number of kernels used, and the number of channels in the input\n          # 3 -> heigth and width of the kernels\n          # padding -> adds a padding of 1 in order to not reduce the size of the image\n          torch.nn.Conv2d(3, 32, 3, padding=1),    #convolutional layer\n          nn.BatchNorm2d(32),                        #normalization layer, normalizes the activations in order to speed up training\n          nn.ReLU(),                               #activation layer, uses Rectified Linear Unit\n          torch.nn.MaxPool2d(kernel_size=2)        #Pooling layer, chooses max value of a 2x2 square\n        ) \n        #data comes out of this layer in the shape 32x16x16\n        \n        self.layer2 = nn.Sequential(\n          torch.nn.Conv2d(32, 64, 3, padding=1),\n          nn.BatchNorm2d(64),\n          nn.ReLU(),\n          torch.nn.MaxPool2d(kernel_size=2)\n        )\n        #data comes out of this layer in the shape 64x8x8\n        \n        self.layer3 = nn.Sequential(\n          torch.nn.Conv2d(64, 128, 3, padding=1),\n          nn.BatchNorm2d(128),\n          nn.ReLU(),\n          torch.nn.MaxPool2d(kernel_size=2)\n        )\n        #data comes out of this layer in the shape 128x4x4\n        \n        self.layer4 = nn.Sequential(\n          torch.nn.Conv2d(128, 128, 3, padding=1),\n          nn.BatchNorm2d(128),\n          nn.ReLU(),\n          nn.AvgPool2d(2)\n        ) \n        #data comes out of this layer in the shape 128x2x2\n\n        self.fc1 = torch.nn.Linear(2*2*128, 256)\n        self.fc2 = torch.nn.Linear(256, 2)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = F.dropout(x, p=0.25)                  #applies a dropout layer, with 0.25 probability of ignoring each element of the input tensor.\n        x = self.layer2(x)\n        x = F.dropout(x, p=0.25)\n        x = self.layer3(x)\n        x = F.dropout(x, p=0.25)    \n        x = self.layer4(x)\n        x = F.dropout(x, p=0.25)  \n        x = x.reshape(x.size(0), -1)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, p=0.25)\n        out = self.fc2(x)\n        return out\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_model(model, epochs=0, val_loss=-1):\n  '''function to save the state of the model, the state of the optimizer, number of current epochs and validation loss'''\n  \n  checkpoint = {'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), \n              'epochs': epochs, 'val_loss': val_loss}\n  torch.save(checkpoint,'../checkpoint.pt')\n  print(\"Model Saved\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_model(checkpoint):\n    '''function to load a model and a optimizer with the respective state dicts,\n    returning them and the lowest validation loss associated with them'''\n    model = CactusNetwork().to(device)\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    \n    model.load_state_dict(checkpoint['model_state_dict'])\n    val_loss = checkpoint['val_loss']\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    \n    return model, optimizer, val_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntry:                   #if there is a file with a checkpoint of trainning load it and use its data\n\n    #load te checkpoint from the file located in the workspace\n    checkpoint = torch.load('../checkpoint.pt')\n    print(\"Loading file\")\n    model, optimizer, val_loss = load_model(checkpoint)\n    \n    # track change in validation loss\n    valid_loss_min = val_loss\n    \n    \nexcept FileNotFoundError:   #if there is no file with a checkpoint of trainning, start fresh\n    print(\"Starting fresh\")\n    \n    #instatiate the model\n    model = CactusNetwork().to(device)\n\n    #define the optimizer as Adam, give him the model's parameters and define the learning rate as 0.001\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    # track change in validation loss\n    valid_loss_min = np.Inf\n    \n\n#define the weights to be used in the various classes\nclass_weights = torch.FloatTensor([1,0.5]).cuda()\n#define the loss as Cross Entropy Loss\ncriterion = nn.CrossEntropyLoss(class_weights)\n\n#schedule the lr to decrease by half after every 3 epochs without improvement\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=5, verbose=True, min_lr=0.00001)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"# number of epochs to train the model\nn_epochs = 40\n\n# number of epochs without improvement that triggers an early stop in trainning\nearly_stop = 12\nes_counter = 0\n\n\nfor epoch in range(1, n_epochs+1):\n\n    # keep track of training and validation loss\n    train_loss = 0.0\n    valid_loss = 0.0\n    accuracy=0.0\n    \n    ###################\n    # train the model #\n    ###################\n    model.train()\n    for batch_number, (data, target) in enumerate(train_loader):\n        data = data.to(device)\n        target = target.to(device)\n        # move tensors to GPU if CUDA is available\n        # clear the gradients of all optimized variables\n        optimizer.zero_grad()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data)\n        # calculate the batch loss\n        loss = criterion(output, target)\n        # backward pass: compute gradient of the loss with respect to model parameters\n        loss.backward()\n        # perform a single optimization step (parameter update)\n        optimizer.step()\n        # update training loss\n        train_loss += loss.item()*data.size(0)\n        #give feedback on current batch of the epoch\n        if batch_number%50 == 0:\n            print(\"batch number: {}\".format(batch_number))\n            \n    ######################    \n    # validate the model #\n    ######################\n    model.eval()\n    for data, target in val_loader:\n        data = data.to(device)\n        target = target.to(device)\n        # move tensors to GPU if CUDA is available\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data)\n        # calculate the batch loss\n        loss = criterion(output, target)\n        # update average validation loss \n        valid_loss += loss.item()*data.size(0)\n        _,pred=torch.max(output,1)\n        accuracy += torch.sum(pred==target.data)\n    \n    # calculate average losses\n    train_loss = train_loss/len(train_loader.dataset)\n    valid_loss = valid_loss/len(val_loader.dataset)\n    accuracy = accuracy.double()/len(val_loader.dataset)\n    \n    # confirm if the validation loss is decreasing in order to reduce lr\n    scheduler.step(valid_loss)\n        \n    # print training/validation statistics \n    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}\\tAccuracy: {:.6f}'.format(\n        epoch, train_loss, valid_loss,accuracy))\n    \n    # save model if validation loss has decreased, if not increase early stop counter \n    if valid_loss < valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n            valid_loss_min, valid_loss))\n        save_model(model, epoch, valid_loss)\n        valid_loss_min = valid_loss\n        es_counter = 0\n    else:\n        es_counter+=1\n    \n    #if certain number of epochs have passed without improvement stop the trainning\n    if es_counter >= early_stop:\n        print(\"\\n\\nEarly stop, no improvements in {} epochs\".format(early_stop))\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/sample_submission.csv')\n\ndataset_test = CactusDataset(df_data=sub, data_dir=test_dir, transform=val_transforms)\ntest_loader = DataLoader(dataset = dataset_test, batch_size=32, shuffle=False, num_workers=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()\n\npreds = []\nfor batch_number, (data, target) in enumerate(test_loader):\n    data, target = data.cuda(), target.cuda()\n    output = model(data)\n\n    pr = output[:,1].detach().cpu().numpy()\n    for i in pr:\n        preds.append(i)\n\nsub['has_cactus'] = preds\nsub.to_csv('sub.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}